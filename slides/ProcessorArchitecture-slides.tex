%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%
%%%% This text file is part of the source of slides for
%%%% `The Art of HPC, vol 1: The Science of Computing'
%%%% by Victor Eijkhout, copyright 2012-2024
%%%%
%%%% slides for architecture lecture
%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{numberedframe}{Justification}
The performance of a parallel code has as one component
the behaviour of the single processor or single-threaded code. 
In this section we discuss the basics of how a processor
executes instructions, and how it handles the data
these instructions operate on.
\end{numberedframe}

\Level 1 {Structure of a modern processor}

\begin{numberedframe}{Von Neumann machine}

The ideal processor:
\begin{itemize}
\item (Stored program)
\item An instruction contains the operation and two operand locations
\item Processor decodes instruction, gets operands, computes and writes back the result
\item Repeat
\end{itemize}

\end{numberedframe}

\begin{numberedframe}{The actual state of affairs}

  \begin{itemize}
  \item Single instruction stream versus multiple cores~/ floating point units
  \item Single instruction stream versus Instruction Level Parallelism
  \item Unit-time-addressable memory versus large latencies
  \end{itemize}
  Modern processors contain lots of magic to make them seem like Von Neumann machines.
\end{numberedframe}

\begin{numberedframe}{Complexity measures}
  Traditional: processor speed was paramount. Operation counting.

  Nowadays: memory is slower than processors 

    \begin{block}{This course}

      Study data movement aspects\\
      Algorithm design for processor reality
    \end{block}
\end{numberedframe}

\begin{numberedframe}{A first look at a processor}
  \includegraphics[scale=.13]{sandybridge-eightcore}
\end{numberedframe}

\begin{numberedframe}{Structure of a core}
  \includegraphics[scale=.4]{sandybridge_pipeline}
\end{numberedframe}

\begin{numberedframe}{Motivation for pipelining}
  An operation consists of several stages.\\
  Addition:
  \begin{itemize}
\item Decoding the instruction  operands.
\item Data fetch into register
\item Aligning the exponents:
\[ 
\begin{array}{ll}
.35\times 10^{-1}\,+\, .6\times 10^{-2}&\hbox{becomes}\\
.35\times 10^{-1}\,+\, .06\times 10^{-1}.
\end{array}
\]
\item Adding mantissas, giving  $.41$.
\item Normalizing the result, giving $.41\times 10^{-1}$.
\item Storing the result.
\end{itemize}
\indextermbus{pipeline}{stages}
\end{numberedframe}

\begin{numberedframe}{Pipelining, pictorially}
  Discrete hardware for each stage:
  
    \includegraphics[scale=.1]{pipeline}
\end{numberedframe}

\begin{numberedframe}{Analysis}
Operation timing:
\[ 
\begin{cases}
  n&\hbox{operations}\\ \ell&\hbox{number of stages}\\ \tau&\hbox{clock cycle}
\end{cases} \Rightarrow
t(n)=n\ell\tau
\]
With pipelining:
\[ t(n)=[s+\ell+n-1]\tau \]
where $s$ is a setup cost

$\Rightarrow$~Asymptotic speedup is $\ell$

$n_{1/2}$: value for which speedup is $\ell/2$
\end{numberedframe}

\begin{numberedframe}{Applicability of pipelining}
  Pipelining works for:\\
  vector addition/multiplication

  Division/square root maybe pipelined, but much slower
\end{numberedframe}

\begin{numberedframe}{Recurrences}
Pipelining does not immediately work:
\begin{verbatim}
for (i) {
  x[i+1] = a[i]*x[i] + b[i];
}
\end{verbatim}
Transform:
\[
\begin{array}{rl}
  x_{n+2}=&a_{n+1}x_{n+1}+b_{n+1}\\
    &=a_{n+1}(a_nx_n+b_n)+b_{n+1}\\
    &=a_{n+1}a_nx_n + a_{n+1}b_n+b_{n+1}
\end{array}
\]
\end{numberedframe}

\begin{numberedframe}{Instruction pipeline}
  \begin{itemize}
  \item Instruction-Level Parallelism: more general notion of 
    independent instructions
  \item Requires independent instructions
  \item As frequency goes up, pipeline gets longer: more demands on compiler
  \end{itemize}
\end{numberedframe}

\begin{numberedframe}{Instruction-Level Parallelism}
  \begin{itemize}
  \item multiple-issue of independent instructions
  \item branch prediction and speculative execution
  \item out-of-order execution
  \item prefetching
  \end{itemize}
  Problems: complicated circuitry, hard to maintain performance
\end{numberedframe}

\begin{numberedframe}{Implications}
  \begin{itemize}
  \item Long pipeline needs many independent instructions:\\
    demands on compiler
  \item Conditionals break the stream of independent instructions
    \begin{itemize}
    \item Processor tries to predict branches
    \item \indexterm{branch misprediction penalty}:\\
      pipeline needs to be flushed and refilled
    \item avoid conditionals in inner loops!
    \end{itemize}
  \end{itemize}
\end{numberedframe}

\begin{numberedframe}{Instructions}
  \begin{itemize}
  \item Addition/multiplication: pipelined
  \item Division (and square root): much slower
\begin{lstlisting}
for ( i )
   a[i] = b[i] / c
\end{lstlisting}
Can you improve on this?
\item \acf{FMA} \lstinline{ s += a*b}\\
  where can you use this?
  \end{itemize}
\end{numberedframe}

\begin{numberedframe}{Peak performance}
\small
Performance is a function of
\begin{itemize}
\item Clock frequency,
\item SIMD width
\item Load/store unit behavior
\end{itemize}

Floating point capabilities of several processor architectures\\
 DAXPY cycle number for 8 operands  

  \begin{tabular}{lllll}
Processor&year&add/mult/fma units  &daxpy cycles\\
         &    &(count$\times$width)&(arith vs load/store)\\
\toprule
MIPS R10000       &1996 &$1\times1+1\times1+0$ &8/24 \\
Alpha EV5         &1996 &$1\times1+1\times1+0$ &8/12 \\
IBM Power5        &2004 &$0+0+2\times1       $ &4/12 \\
AMD Bulldozer     &2011 &$2\times2+2\times2+0$ &2/4  \\
Intel Sandy Bridge&2012 &$1\times4+1\times4+0$ &2/4  \\
Intel Haswell     &2014 &$0+0+2\times 4      $ &1/2  \\
%Intel Woodcrest, AMD Barcelona&2 add + 2 mul&4 \\  %SIMD FADD, FMUL
%IBM POWER4, POWER5, POWER6&    2 FMA & 4 \\
%IBM BG/L, BG/P & 1 SIMD FMA & 4 \\
%SPARC IV & 1 add + 1 mul& 2 \\
%Itanium2 &  2 FMA & 4 
  \end{tabular}
\end{numberedframe}

\Level 1 {Memory hierarchy: caches, register, TLB.}

\begin{numberedframe}{The Big Story}
  \begin{itemize}
  \item DRAM memory is slow, so let's put small SRAM close to the processor
  \item This helps if data is reused 
  \item Does the algorithm have reuse?
  \item Does the implementation reuse data?
  \end{itemize}
\end{numberedframe}

\begin{numberedframe}{Bandwidth and latency}
  Important theoretical concept:
  \begin{itemize}
  \item \indexterm{latency} is delay between request for data and availability
  \item \indexterm{bandwidth} is rate at which data arrives thereafter
  \end{itemize}
\end{numberedframe}

\begin{numberedframe}{Memory hierarchy}
  \includegraphics[scale=.1]{hierarchysb}  
\end{numberedframe}

\Level 2 {Registers}

\begin{numberedframe}{Computing out of registers}
\begin{verbatim}
a := b + c
\end{verbatim}
\begin{itemize}
\item load the value of \n{b} from memory into a \indexterm{register},
\item load the value of \n{c} from memory into another register,
\item compute the sum and write that into yet another register, and
\item write the sum value back to the memory location of~\n{a}.
\end{itemize}
\end{numberedframe}

\begin{numberedframe}{Register usage}
  Assembly code\\
  (note: Intel two-operand syntax)
\begin{verbatim}
addl	%eax, %edx
\end{verbatim}  
\begin{itemize}
\item Registers are named
\item Can be explicitly addressed by the programmer
\item \ldots as opposed to caches.
\item Assembly coding or inline assembly (compiler dependent)
\item \ldots but typically generated by compiler
\end{itemize}
\end{numberedframe}

\begin{numberedframe}{Examples of register usage}
1. Resident in register
\begin{verbatim}
a := b + c
d := a + e
\end{verbatim}
\n{a} stays resident in register, avoid store and load  

2. subexpression elimination:
\begin{verbatim}
t1 = sin(alpha) * x + cos(alpha) * y;
t2 = -cos(alpha) * x + sin(alpha) * y;
\end{verbatim}
becomes:
\begin{verbatim}
s = sin(alpha); c = cos(alpha);
t1 = s * x + c * y;
t2 = -c * x + s * y
\end{verbatim}
often done by compiler
\end{numberedframe}

\Level 2 {Caches}

\begin{numberedframe}{Cache basics}
Fast SRAM in between memory and registers: mostly serves data reuse
\begin{verbatim}
... = ... x ..... // instruction using x
.........         // several instructions not involving x
... = ... x ..... // instruction using x
\end{verbatim}
\begin{itemize}
\item load \n{x} from memory into cache, and from cache into register;
  operate on it;
\item do the intervening instructions;
\item request \n{x} from memory, but since it is still in the cache,
  load it from the cache into register; operate on it.
\item essential concept: \indexterm{data reuse}
\end{itemize}
\end{numberedframe}

\begin{numberedframe}{Cache levels}
  \index{cache!levels}
  \begin{itemize}
  \item Levels 1,2,3(,4): L1, L2, etc.
  \item Increasing size, increasing latency, decreasing bandwidth
  \item (Note: L3/L4 can be fairly big; beware benchmarking)
  \item Cache hit / cache miss: one level is consulted, then the next
  \item L1 has separate data / instruction cache, other levels mixed
  \item Caches do not have enough bandwidth to serve the processor:
    coding for reuse on all levels.
  \end{itemize}
\end{numberedframe}

\begin{numberedframe}{Cache misses}
  \index{cache!miss}
  \begin{itemize}
  \item Compulsory miss: first time data is referenced
  \item Capacity miss: data was in cache, but has been flushed (overwritten)
    by LRU policy
  \item Conflict miss: two items get mapped to the same cache location,
    even if there are no capacity problems
  \item Invalidation miss: data becomes invalid because of activity of another core
  \end{itemize}  
\end{numberedframe}

\begin{numberedframe}{Cache hits}
  \begin{itemize}
  \item Data has been requested,
    used a second time: temporal locality
  \item $\Rightarrow$ Can't wait too long between uses
  \item (Data can be loaded because it's close
    to data requested: spatial locality. Later.)
  \end{itemize}
\end{numberedframe}

\begin{numberedframe}{Capacity miss}
  \includegraphics[scale=.5]{capacity_miss}

  (Why is that last block going where it is going?)
\end{numberedframe}

\begin{numberedframe}{Cache capacity}
  \begin{itemize}
  \item Loading data multiple times
  \item LRU: oldest item evicted if needed
  \item Reuse if not too much data
  \end{itemize}
\begin{lstlisting}
for ( lots of times ) // sequential loop
    load and process data // probably parallel loop
\end{lstlisting}
\end{numberedframe}

\begin{numberedframe}{Illustration of capacity}
  \includegraphics[scale=.4]{cacheoverflow}
\end{numberedframe}

\begin{numberedframe}{Replacement policies}
  What determines where new data goes~/\\
  what old data is overwritten?

  \begin{itemize}
  \item \acf{LRU}: most common
  \item \acf{FIFO}: IBM Power4. Not a good idea.
  \item Random Replacement. Sometimes used.
  \end{itemize}

  It's actually more subtle than pure \ac{LRU}~\ldots
\end{numberedframe}

\begin{numberedframe}{Cache lines}
  \begin{itemize}
  \item Memory requests go by byte or word
  \item Memory transfers go by \indextermbus{cache}{line}:\\
    typically 64 bytes~/ 8 double precision numbers
  \item Cache line transfer costs bandwidth
  \item $\Rightarrow$ important to use all elements
  \end{itemize}
\end{numberedframe}

\begin{numberedframe}{Effects of striding}
  \begin{itemize}
  \item Always 8 numbers transferred
  \item With stride~$s>1$: $8/s$ elements used
  \item Loss of efficiency if bandwidth-limited
  \end{itemize}
\end{numberedframe}

\begin{numberedframe}{Cache line use}
\begin{verbatim}
for (i=0; i<N; i++)
  ... = ... x[i] ...
\end{verbatim}
  \includegraphics[scale=.07]{stride-1}
\begin{verbatim}
for (i=0; i<N; i+=stride)
  ... = ... x[i] ...
\end{verbatim}
  \includegraphics[scale=.07]{stride-3}
\end{numberedframe}

\begin{numberedframe}{Stride effects}
\small
\begin{verbatim}
for (i=0,n=0; i<L1WORDS; i++,n+=stride)
  array[n] = 2.3*array[n]+1.2;
\end{verbatim}
  \includegraphics[scale=.4]{cacheline8}
\end{numberedframe}

\begin{numberedframe}{Cache mapping}
  Cache is smaller than memory, so we need a mapping scheme\\
  memory address $\mapsto$ cache address
  \begin{itemize}
  \item Ideal: any address can go anywhere; LRU policy for replacement
  \item pro: optimal; con: slow, expensive to manufacture
  \item Simple: direct mapping by truncating addresses
  \item pro: fast and cheap; con: I'll show you in a minute
  \item Practical: limited associativity\\
    `enough but not too much'
  \end{itemize}
\end{numberedframe}

\begin{numberedframe}{Direct mapping}
  \index{cache!direct mapping}
  \includegraphics[scale=.06]{directmap}

Direct mapping of 32-bit addresses into a 64K cache
\begin{itemize}
\item Use last number of bits to find cache address
\item If you traverse an array, a contiguous chunk will be mapped
  to cache without conflict.
\item If (memory) addresses are cache size apart, they get mapped
  to the same cache location
\end{itemize}
\end{numberedframe}

\begin{numberedframe}{Conflicts}
  \includegraphics[scale=.1]{directmapconflict}  

  Mapping conflicts in a direct-mapped cache.
\end{numberedframe}

\begin{numberedframe}{The problem with direct mapping}
\begin{verbatim}
real*8 A(8192,3);
do i=1,512
  a(i,3) = ( a(i,1)+a(i,2) )/2
end do
\end{verbatim}
In each iteration 3 elements map to the same cache location:\\
constant overwriting (`eviction', \indextermbus{cache}{thrasing}):\\
low performance
\end{numberedframe}

\begin{numberedframe}{Associative cache mapping}  
  \index{cache!associative mapping}
  \begin{itemize}
  \item Allow each memory address to go to multiple (but not all) cache addresses;
    typically 2,4,8
  \item Prevents problems with multiple arrays
  \item Reasonable fast
  \item Often lower associativity for L1 than L2, L3
  \end{itemize}
  \begin{tabular}{|l|ll|}
    \toprule
    Associativity&L1&L2\\
    \midrule
    Intel (Woodcrest)&8&8\\
    AMD (Bulldozer)&2&8\\
    \bottomrule
  \end{tabular}
\end{numberedframe}

\begin{numberedframe}{Associativity}
  \includegraphics[scale=.4]{associativity}

  Associative cache structure
\end{numberedframe}

\begin{numberedframe}{Illustration of associativity}
\includegraphics[scale=.1]{assoc-mapping}

Two caches of 12 elements: direct mapped (left) and 3-way associative (right)

Direct map: 0--12 is conflict\\ Associative: no conflict
\end{numberedframe}

\begin{numberedframe}{Associativity in practice}
\[ \forall_j\colon y_j= y_j+\sum_{i=1}^mx_{i,j} \]
  \includegraphics[scale=.4]{l1_assoc}

The number of L1 cache misses and the number of cycles for
    each $j$ column accumulation, vector length~$4096$
\end{numberedframe}

\begin{numberedframe}{One remedy}
Do not user powers of 2.

  \includegraphics[scale=.35]{l1_assocshift}

The number of L1 cache misses and the number of cycles for
    each $j$ column accumulation, vector length~$4096+8$
\end{numberedframe}

\begin{numberedframe}{Exercise}
  Write a small cache simulator in your favorite language. Assume a
  $k$-way associative cache of 32 entries and an architecture with 16
  bit addresses.  
\end{numberedframe}

\begin{numberedframe}{Exercise: vectorsum}
  \begin{itemize}
  \item Compare sequential performance to single-threaded OMP
  \item For some problem sizes observe a difference in performance
  \item Use Intel option \n{-qopt-report=3} and inspect the report.
  \item Compare different compilers: Intel 19 behaves differently from 24! Also gcc13.
  \end{itemize}
\begin{lstlisting}
for ( int iloop=0; iloop<nloops; ++iloop ) {
  for ( int i=0; i<vectorsize; ++i ) {
    outvec[i] += invec[i]*loopcoeff[iloop];
  }
}
\end{lstlisting}
Analyze and report
\end{numberedframe}

\Level 2 {More memory system topics}

\begin{numberedframe}{Bandwidth / latency}
  Simple model for sending $n$ words: \[ t=\alpha + \beta n \]
  Quoted bandwidth figures are always optimistic:
  \begin{itemize}
  \item bandwidth shared between cores\\
    not enough bandwidth for all cores:\\
    $\Rightarrow$~speedup less than linear
  \item bandwidth wasted on coherence
  \item NUMA: pulling data from other socket
  \item assumes optimal scheduling of DRAM banks
  \end{itemize}
\end{numberedframe}

\begin{numberedframe}{Prefetch}
  \begin{itemize}
  \item Do you have to wait for every item from memory?
  \item Memory controller can infer streams: prefetch
  \item Sometimes controllable through assembly, directives, libraries (AltiVec)
  \item One form of latency hiding
  \end{itemize}
\includegraphics[scale=.06]{prefetch}
\end{numberedframe}

\begin{numberedframe}{Memory pages}
  Memory is organized in pages:
  \begin{itemize}
  \item Translation between logical address, as used by program,
    and physical in memory
  \item This serves virtual memory and relocatable code
  \item so we need another translation stage.
  \end{itemize}
\end{numberedframe}

\begin{numberedframe}{Page translation: TLB}
  \begin{itemize}
  \item General page translation: slowish and expensive
  \item \indexac{TLB} is a small list of frequently used pages
  \item Example of spatial locality: items on an already referenced page
    are found faster
  \end{itemize}
\end{numberedframe}

\begin{numberedframe}{TLB misses}
\small
\begin{verbatim}
#define INDEX(i,j,m,n) i+j*m
array = (double*) malloc(m*n*sizeof(double));
/* traversal #2 */
for (i=0; i<m; i++)
  for (j=0; j<n; j++)
    array[INDEX(i,j,m,n)] = array[INDEX(i,j,m,n)]+1;
\end{verbatim}
  \includegraphics[scale=.35]{tlb_row}
\end{numberedframe}

\begin{numberedframe}{TLB hits}
\small
\begin{verbatim}
#define INDEX(i,j,m,n) i+j*m
array = (double*) malloc(m*n*sizeof(double));
/* traversal #1 */
for (j=0; j<n; j++)
  for (i=0; i<m; i++)
    array[INDEX(i,j,m,n)] = array[INDEX(i,j,m,n)]+1;
\end{verbatim}
  \includegraphics[scale=.35]{tlb_col}
\end{numberedframe}

\begin{numberedframe}{Little's Law}
  \begin{itemize}
  \item Item loaded from memory, processed, new item loaded in response
  \item But this can only happen after latency wait
  \item Items during latency are independent, therefore
    \[ \mathrm{Concurrency}=\mathrm{Bandwidth}\times \mathrm{Latency}. \]
  \end{itemize}
  \includegraphics[scale=.1]{little}
\end{numberedframe}

\Level 1 {Multicore issues}

\begin{numberedframe}{Why multicore}
  Quest for higher performance:
  \begin{itemize}
  \item Not enough instruction parallelism for long pipelines
  \item Two cores at half speed more energy-efficient than one at full speed.
  \end{itemize}
  Multicore solution:
  \begin{itemize}
  \item More theoretical performance
  \item Burden for parallelism is now on the programmer
  \end{itemize}
\end{numberedframe}

\begin{numberedframe}{Dennard scaling}
Scale down feature size by~$s$:
\[
\begin{array}{|l|c|}\hline
\hbox{Feature size}&\sim s\\
\hbox{Voltage}&\sim s\\
\hbox{Current}&\sim s \\ 
\hbox{Frequency}&\sim s\inv\\
\hline
\end{array}
\]  
Miracle conclusion:
\[ \hbox{Power} = V\cdot I \sim s^2; \hbox{Power density}\sim 1 \]
Everything gets better, cooling problem stays the same

Opportunity for more components, higher frequency
\end{numberedframe}

\begin{numberedframe}{Dynamic power}
\begin{equation}
\begin{array}{|l|l|} \hline
\hbox{Charge}&q=CV\\
\hbox{Work}&W=qV=CV^2\\
\hbox{Power}&W/\hbox{time}=WF=CV^2F \\ \hline
\end{array}
\label{eq:power}
\end{equation}  
Two cores at half frequency:
\[ \left.
\begin{array}{c}
C_{\mathrm{multi}} = 2C\\
F_{\mathrm{multi}} = F/2\\
V_{\mathrm{multi}} = V/2\\
\end{array}\right\} \Rightarrow
P_{\mathrm{multi}} = P/4.
\]
Same computation, less power
\end{numberedframe}

\begin{numberedframe}{Multicore caches}
  \includegraphics[scale=.12]{cache-hierarchy}
\end{numberedframe}

\begin{numberedframe}{The coherence problem}
  \includegraphics[scale=.5]{coherence_miss}
\end{numberedframe}

\begin{numberedframe}{Cache coherence}
\acf{MSI} coherence protocol:
\begin{description}
\item [Modified:] the cacheline has been modified
\item [Shared:] the line is present in at least one cache and is unmodified.
\item [Invalid:] the line is not present, or it
  is present but a copy in another cache has been modified.
\end{description}
\end{numberedframe}

\begin{numberedframe}{Coherence issues}
  \begin{itemize}
  \item Coherence is automatic, so you don't have to worry about it\ldots
  \item \ldots except when it saps performance
  \item Beware false sharing\\
    writes to different elements of a cache line
  \end{itemize}
\end{numberedframe}

\begin{numberedframe}{Balance analysis}
  \begin{itemize}
  \item Sandy Bridge core can aborb 300 GB/s
  \item 4 DDR3/1600 channels provide 51 GB/s, difference has to come from reuse
  \item It gets worse: latency 80ns, bandwidth 51 GB/s, \\
    Little's law: parallelism 64 cache lines
  \item However, each core only has 10 line fill buffers,\\
    so we need 6--7 cores to provide the data for one core
  \item Power: cores are 72\%, uncore 17, DRAM~11.
  \item Core power goes 40\% to instruction handling, not arithmetic
  \item Time for a redesign of processors and programming; see my research presentation
  \end{itemize}
\end{numberedframe}

\Level 1 {Programming strategies for performance}

\begin{numberedframe}{How much performance is possible?}
  Performance limited by
  \begin{itemize}
  \item Processor peak performance: absolute limit
  \item Bandwidth: linear correlation with performance
  \end{itemize}
  \indexterm{Arithmetic intensity}: ratio of operations per transfer

  If AI high enough: processor-limited\\
  otherwise: bandwidth-limited
\end{numberedframe}

\begin{numberedframe}
  Performance depends on algorithm:

  \includegraphics[scale=.1]{roofline1}
\end{numberedframe}

\begin{numberedframe}
  Insufficient utilization of functional units:

  \includegraphics[scale=.1]{roofline2}
\end{numberedframe}

\begin{numberedframe}
  Imperfect data transfer:

  \includegraphics[scale=.1]{roofline3}
\end{numberedframe}

\begin{numberedframe}{Spatial and temporal locality}
  Temporal locality: use an item, use it again but from cache\\
  efficient because second transfer cheaper.

  Spatial locality: use an item, then use one `close to it'\\
  (for instance from same cacheline)\\
  efficient because item is already reachable even though not used before.
\end{numberedframe}

\begin{numberedframe}{Architecture aware programming}
  \begin{itemize}
  \item Cache size: block loops
  \item pipelining and vector instructions: expose streams of instructions
  \item reuse: restructure code (both loop merge and splitting, unroll
  \item TLB: don't jump all over memory
  \item associativity: watch out for powers of 2
  \end{itemize}
\end{numberedframe}

\begin{numberedframe}{Loop blocking}
Multiple passes over data
\begin{verbatim}
for ( k< small bound )
  for ( i < N )
    x[i] = f( x[i], k, .... )
\end{verbatim}
Block to be cache contained
\begin{verbatim}
for ( ii < N; ii+= blocksize )
  for ( k< small bound )
    for ( i=ii; i<ii+blocksize; i++ )
      x[i] = f( x[i], k, .... )
\end{verbatim}
This requires independence of operations
\end{numberedframe}

\begin{numberedframe}{The ultimate in performance programming: DGEMM}
  Matrix-matrix product $C=A\cdot B$
  \[ \forall_i\forall_j\forall_k\colon c_{ij}\mathop{+=} a_{ik}b_{kj} \]
  \begin{itemize}
  \item Three independent loop $i,j,k$
  \item all three blocked $i',j',k'$
  \item Many loop permutations, blocking factors to choose
  \end{itemize}
\end{numberedframe}

\begin{numberedframe}{DGEMM variant}
Inner products
\begin{verbatim}
for ( i )
  for ( j )
    for ( k )
      c[i,j] += a[i,k] * b[k,j]
\end{verbatim}
\end{numberedframe}

\begin{numberedframe}{DGEMM variant}
Outer product: updates with low-rank columns-times-vector
\begin{verbatim}
for ( k )
  for ( i )
    for ( j )
      c[i,j] += a[i,k] * b[k,j]
\end{verbatim}
\end{numberedframe}

\begin{numberedframe}{DGEMM variant}
Building up rows by linear combinations
\begin{verbatim}
for ( i )
  for ( k )
    for ( j )
      c[i,j] += a[i,k] * b[k,j]
\end{verbatim}
Exchanging $i,j$: building up columns
\end{numberedframe}

\begin{numberedframe}{Rank 1 updates}
  \[ C_{**} = \sum_k A_{*k}B_{k*} \]
  \includegraphics[scale=.07]{gotoblas1}
\end{numberedframe}

\begin{numberedframe}{Matrix-panel multiply}
  Block of $A$ times `sliver' of~$B$

  \includegraphics[scale=.08]{gotoblas2}  
\end{numberedframe}

\begin{numberedframe}{Inner algorithm}
For inner $i$:
\begin{verbatim}
// compute C[i,*] :
for k:
   C[i,*] = A[i,k] * B[k,*]
\end{verbatim}
  \includegraphics[scale=.1]{gotoblas3}
\end{numberedframe}

\begin{numberedframe}{Tuning}
For inner $i$:
\begin{verbatim}
// compute C[i,*] :
for k:
   C[i,*] += A[i,k] * B[k,*]
\end{verbatim}
\begin{itemize}
\item \verb+C[i,*]+ stays in register
\item \verb+A[i,k]+ and \verb+B[k,*]+ stream from L1
\item blocksize of $A$ for L2 size
\item $A$ stored by rows to prevent TLB problems
\end{itemize}
\end{numberedframe}

\begin{numberedframe}{Cache-oblivious programming}
Observation: recursive subdivision will ultimately
make a problem small / well-behaved enough
  \includegraphics[scale=.08]{oblivious1}
\end{numberedframe}

\begin{numberedframe}{Cache-oblivious matrix-matrix multiply}
  \[ 
  \begin{pmatrix}
    C_{11}&C_{12}\\ C_{21}&C_{22}
  \end{pmatrix}
  =
  \begin{pmatrix}
    A_{11}&A_{12}\\ A_{21}&A_{22}
  \end{pmatrix}
  \begin{pmatrix}
    B_{11}&B_{12}\\ B_{21}&B_{22}
  \end{pmatrix}
  \]
  with $C_{11}=A_{11}B_{11}+A_{12}B_{21}$

  Recursive approach will be cache contained.\\
  Not as high performance as being cache-aware\ldots  
\end{numberedframe}

\Level 1 {The power question}

\endinput

\begin{numberedframe}{}
  \begin{itemize}
  \item 
  \end{itemize}
\begin{lstlisting}
\end{lstlisting}
\end{numberedframe}

